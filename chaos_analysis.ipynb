{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chaos Theory in Time Series Analysis\n",
    "\n",
    "**A Comprehensive Analysis of Nonlinear Dynamics in Time Series Data**\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the application of chaos theory methods to time series analysis. We explore several key metrics from nonlinear dynamics:\n",
    "\n",
    "1. **Statistical Tests**: BDS test for nonlinearity, Augmented Dickey-Fuller test for stationarity\n",
    "2. **Lyapunov Exponents**: Measuring sensitivity to initial conditions\n",
    "3. **Hurst Exponent**: Detecting long-range dependence\n",
    "4. **Sample Entropy**: Quantifying complexity and regularity\n",
    "5. **Fractal Dimension**: Characterizing geometric complexity\n",
    "6. **Spectral Analysis**: Fourier transform and frequency domain analysis\n",
    "7. **Phase Space Reconstruction**: Visualizing attractors\n",
    "\n",
    "**Author**: [Javihaus](https://github.com/Javihaus)  \n",
    "**Last Updated**: November 2025  \n",
    "**License**: MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistical analysis\n",
    "import statsmodels.tsa.stattools as stat\n",
    "from scipy import stats\n",
    "\n",
    "# Nonlinear dynamics\n",
    "import nolds  # Calculation of Lyapunov exponents, Hurst exponent, entropy\n",
    "\n",
    "# Phase space visualization\n",
    "try:\n",
    "    import pynamical\n",
    "    from pynamical import phase_diagram, phase_diagram_3d\n",
    "    PYNAMICAL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PYNAMICAL_AVAILABLE = False\n",
    "    print(\"Note: pynamical not installed. Phase diagrams will be skipped.\")\n",
    "\n",
    "# Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete.\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration\n",
    "\n",
    "We analyze Google Trends data comparing search interest for \"online shopping\" vs \"shopping mall\" (Spanish: \"Comprar online\" vs \"Centro comercial\"). This dataset spans from 2004 to 2020 and captures an interesting structural shift in consumer behavior—particularly the dramatic impact of the COVID-19 pandemic in early 2020.\n",
    "\n",
    "This type of behavioral time series data is particularly interesting for chaos analysis because:\n",
    "- It reflects complex socioeconomic dynamics\n",
    "- It may exhibit nonlinear regime changes\n",
    "- The COVID shock provides a natural experiment in system perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "DATA_FILE = 'multiTimeline.csv'\n",
    "\n",
    "df = pd.read_csv(DATA_FILE, index_col='Mes', parse_dates=['Mes'])\n",
    "df = df.reset_index()\n",
    "\n",
    "# Rename columns for clarity (Spanish to English)\n",
    "df.columns = ['Date', 'Online_Shopping', 'Shopping_Mall']\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "print(f\"Number of observations: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first and last rows\n",
    "print(\"First 5 rows:\")\n",
    "display(df.head())\n",
    "print(\"\\nLast 5 rows:\")\n",
    "display(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Descriptive Statistics:\")\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visual Exploration of the Time Series\n",
    "\n",
    "Before applying chaos analysis methods, we perform exploratory data analysis to understand the basic characteristics of our time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Time Series Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "# Raw time series\n",
    "ax1 = axes[0]\n",
    "ax1.plot(df['Date'], df['Online_Shopping'], label='Online Shopping', color='#2ecc71', linewidth=1.5)\n",
    "ax1.plot(df['Date'], df['Shopping_Mall'], label='Shopping Mall', color='#3498db', linewidth=1.5)\n",
    "ax1.axvline(pd.Timestamp('2020-03-01'), color='red', linestyle='--', alpha=0.7, label='COVID-19 Impact')\n",
    "ax1.set_ylabel('Search Interest (0-100)', fontsize=11)\n",
    "ax1.set_title('Google Trends: Consumer Shopping Behavior (2004-2020)', fontsize=13, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Log-transformed series\n",
    "ax2 = axes[1]\n",
    "ax2.plot(df['Date'], np.log(df['Online_Shopping']), label='log(Online Shopping)', color='#2ecc71', linewidth=1.5)\n",
    "ax2.plot(df['Date'], np.log(df['Shopping_Mall']), label='log(Shopping Mall)', color='#3498db', linewidth=1.5)\n",
    "ax2.axvline(pd.Timestamp('2020-03-01'), color='red', linestyle='--', alpha=0.7)\n",
    "ax2.set_xlabel('Date', fontsize=11)\n",
    "ax2.set_ylabel('Log Search Interest', fontsize=11)\n",
    "ax2.set_title('Log-Transformed Time Series', fontsize=13, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/time_series_overview.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Online Shopping distribution\n",
    "ax1 = axes[0]\n",
    "ax1.hist(df['Online_Shopping'], bins=30, density=True, alpha=0.6, color='#2ecc71', edgecolor='white')\n",
    "df['Online_Shopping'].plot.kde(ax=ax1, linewidth=2.5, color='#27ae60', label='KDE')\n",
    "ax1.set_xlabel('Search Interest', fontsize=11)\n",
    "ax1.set_ylabel('Density', fontsize=11)\n",
    "ax1.set_title('Distribution: Online Shopping', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "\n",
    "# Shopping Mall distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(df['Shopping_Mall'], bins=30, density=True, alpha=0.6, color='#3498db', edgecolor='white')\n",
    "df['Shopping_Mall'].plot.kde(ax=ax2, linewidth=2.5, color='#2980b9', label='KDE')\n",
    "ax2.set_xlabel('Search Interest', fontsize=11)\n",
    "ax2.set_ylabel('Density', fontsize=11)\n",
    "ax2.set_title('Distribution: Shopping Mall', fontsize=12, fontweight='bold')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/distributions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Normality tests\n",
    "print(\"Shapiro-Wilk Normality Tests:\")\n",
    "for col in ['Online_Shopping', 'Shopping_Mall']:\n",
    "    stat_val, p_val = stats.shapiro(df[col])\n",
    "    print(f\"  {col}: W={stat_val:.4f}, p-value={p_val:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Autocorrelation Analysis\n",
    "\n",
    "Autocorrelation reveals how current values depend on past values. Strong autocorrelation suggests predictable structure, while rapid decay to zero suggests more random behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Lag plots\n",
    "ax1 = axes[0, 0]\n",
    "pd.plotting.lag_plot(df['Online_Shopping'], ax=ax1, c='#2ecc71', alpha=0.6)\n",
    "ax1.set_title('Lag Plot: Online Shopping', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "pd.plotting.lag_plot(df['Shopping_Mall'], ax=ax2, c='#3498db', alpha=0.6)\n",
    "ax2.set_title('Lag Plot: Shopping Mall', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Autocorrelation plots\n",
    "ax3 = axes[1, 0]\n",
    "pd.plotting.autocorrelation_plot(df['Online_Shopping'], ax=ax3, color='#2ecc71')\n",
    "ax3.set_title('Autocorrelation: Online Shopping', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlim(0, 50)\n",
    "\n",
    "ax4 = axes[1, 1]\n",
    "pd.plotting.autocorrelation_plot(df['Shopping_Mall'], ax=ax4, color='#3498db')\n",
    "ax4.set_title('Autocorrelation: Shopping Mall', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlim(0, 50)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('images/autocorrelation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Tests for Time Series Properties\n",
    "\n",
    "Before proceeding with chaos analysis, we test fundamental properties of our time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 BDS Test for Nonlinearity\n",
    "\n",
    "The BDS test (Brock, Dechert, Scheinkman, 1987) tests the null hypothesis that a time series consists of independent and identically distributed (i.i.d.) observations.\n",
    "\n",
    "**Interpretation:**\n",
    "- High BDS statistic and low p-value ($p < 0.05$) → Reject null hypothesis of i.i.d.\n",
    "- Rejection indicates the presence of nonlinear structure (deterministic chaos or nonlinear stochastic dependence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bds_test(series, name):\n",
    "    \"\"\"Perform BDS test and interpret results.\"\"\"\n",
    "    bds_stat, p_val = stat.bds(series)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"BDS Test: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"BDS Statistic: {bds_stat:.4f}\")\n",
    "    print(f\"P-value: {p_val:.4e}\")\n",
    "    \n",
    "    if p_val < 0.05:\n",
    "        print(f\"\\nConclusion: REJECT null hypothesis of i.i.d. (p < 0.05)\")\n",
    "        print(f\"The series exhibits nonlinear dependence structure.\")\n",
    "    else:\n",
    "        print(f\"\\nConclusion: FAIL TO REJECT null hypothesis (p >= 0.05)\")\n",
    "        print(f\"No significant evidence of nonlinear structure.\")\n",
    "    \n",
    "    return bds_stat, p_val\n",
    "\n",
    "bds_online = perform_bds_test(df['Online_Shopping'], 'Online Shopping')\n",
    "bds_mall = perform_bds_test(df['Shopping_Mall'], 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Augmented Dickey-Fuller (ADF) Test for Stationarity\n",
    "\n",
    "The ADF test examines whether a time series has a unit root (non-stationary). \n",
    "\n",
    "**Interpretation:**\n",
    "- If ADF statistic < critical value → Reject null hypothesis → Series is **stationary**\n",
    "- If ADF statistic > critical value → Fail to reject → Series is **non-stationary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_adf_test(series, name):\n",
    "    \"\"\"Perform Augmented Dickey-Fuller test and interpret results.\"\"\"\n",
    "    result = stat.adfuller(series)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ADF Test: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"ADF Statistic: {result[0]:.4f}\")\n",
    "    print(f\"P-value: {result[1]:.4f}\")\n",
    "    print(f\"Lags Used: {result[2]}\")\n",
    "    print(f\"Number of Observations: {result[3]}\")\n",
    "    print(f\"\\nCritical Values:\")\n",
    "    for key, value in result[4].items():\n",
    "        comparison = \"<\" if result[0] < value else \">\"\n",
    "        print(f\"  {key}: {value:.4f} (ADF {comparison} CV)\")\n",
    "    \n",
    "    if result[1] < 0.05:\n",
    "        print(f\"\\nConclusion: REJECT null hypothesis (p < 0.05)\")\n",
    "        print(f\"The series is STATIONARY.\")\n",
    "    else:\n",
    "        print(f\"\\nConclusion: FAIL TO REJECT null hypothesis (p >= 0.05)\")\n",
    "        print(f\"The series is NON-STATIONARY (has unit root).\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "adf_online = perform_adf_test(df['Online_Shopping'], 'Online Shopping')\n",
    "adf_mall = perform_adf_test(df['Shopping_Mall'], 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Maximal Lyapunov Exponent (MLE)\n",
    "\n",
    "The Lyapunov exponent quantifies the rate at which nearby trajectories in phase space diverge or converge. It is the defining characteristic of chaotic systems.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "For a discrete map $x_{n+1} = f(x_n)$:\n",
    "\n",
    "$$\\lambda = \\lim_{n \\to \\infty} \\frac{1}{n} \\sum_{i=0}^{n-1} \\ln |f'(x_i)|$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $\\lambda > 0$: **Chaotic** - nearby trajectories diverge exponentially\n",
    "- $\\lambda = 0$: Marginal stability (bifurcation point)\n",
    "- $\\lambda < 0$: **Stable** - trajectories converge\n",
    "\n",
    "The **Lyapunov time** $\\tau_\\lambda = 1/\\lambda$ indicates the timescale over which predictability is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Rosenstein Algorithm\n",
    "\n",
    "The Rosenstein algorithm (1993) is a robust method for estimating the largest Lyapunov exponent from time series data. It tracks the average divergence of nearest neighbors in reconstructed phase space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lyapunov_rosenstein(series, name):\n",
    "    \"\"\"Compute MLE using Rosenstein algorithm with visualization.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    mle = nolds.lyap_r(series, debug_plot=True, plot_file=ax)\n",
    "    \n",
    "    ax.set_title(f'Lyapunov Exponent Estimation (Rosenstein): {name}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Time Steps', fontsize=11)\n",
    "    ax.set_ylabel('Average Logarithmic Divergence', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/lyapunov_rosenstein_{name.lower().replace(\" \", \"_\")}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Lyapunov Exponent (Rosenstein): {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MLE (lambda): {mle:.6f}\")\n",
    "    \n",
    "    if mle > 0:\n",
    "        lyap_time = 1 / mle\n",
    "        print(f\"Lyapunov Time: {lyap_time:.2f} time units\")\n",
    "        print(f\"\\nInterpretation: POSITIVE MLE indicates chaotic dynamics.\")\n",
    "        print(f\"The system is sensitive to initial conditions.\")\n",
    "    else:\n",
    "        print(f\"\\nInterpretation: NEGATIVE/ZERO MLE indicates non-chaotic dynamics.\")\n",
    "        print(f\"The system is NOT sensitive to initial conditions.\")\n",
    "    \n",
    "    return mle\n",
    "\n",
    "mle_mall = compute_lyapunov_rosenstein(df['Shopping_Mall'].values, 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_online = compute_lyapunov_rosenstein(df['Online_Shopping'].values, 'Online Shopping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Wolf Algorithm\n",
    "\n",
    "The Wolf algorithm (1985) is a classical method that follows a fiducial trajectory and its nearest neighbor, replacing the neighbor when they diverge beyond a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phase_space_reconstruction(data, m, tau):\n",
    "    \"\"\"\n",
    "    Reconstruct phase space using time-delay embedding (Takens' theorem).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Original time series\n",
    "    m : int\n",
    "        Embedding dimension\n",
    "    tau : int\n",
    "        Time delay\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X : ndarray\n",
    "        Reconstructed phase space matrix (m x M)\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    M = N - (m - 1) * tau\n",
    "    X = np.zeros((m, M))\n",
    "    for j in range(M):\n",
    "        for i in range(m):\n",
    "            X[i, j] = data[i * tau + j]\n",
    "    return X\n",
    "\n",
    "\n",
    "def lyapunov_wolf(data, m=1, tau=1, P=5):\n",
    "    \"\"\"\n",
    "    Compute Maximal Lyapunov Exponent using Wolf's algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Time series data\n",
    "    m : int\n",
    "        Embedding dimension\n",
    "    tau : int\n",
    "        Time delay\n",
    "    P : int\n",
    "        Mean orbital period\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    lambda_1 : float\n",
    "        Estimated maximal Lyapunov exponent\n",
    "    lmd : ndarray\n",
    "        Running estimate of MLE\n",
    "    \"\"\"\n",
    "    N = len(data)\n",
    "    min_point = 1\n",
    "    MAX_ITERATIONS = 5\n",
    "    \n",
    "    # Reconstruct phase space\n",
    "    Y = phase_space_reconstruction(data, m, tau)\n",
    "    M = N - (m - 1) * tau\n",
    "    \n",
    "    # Calculate distance statistics\n",
    "    max_d = 0\n",
    "    min_d = 1e100\n",
    "    avg_dd = 0\n",
    "    \n",
    "    for i in range(M - 1):\n",
    "        for j in range(i + 1, M):\n",
    "            d = np.sqrt(np.sum((Y[:, i] - Y[:, j]) ** 2))\n",
    "            max_d = max(max_d, d)\n",
    "            min_d = min(min_d, d)\n",
    "            avg_dd += d\n",
    "    \n",
    "    avg_d = 2 * avg_dd / (M * (M - 1))\n",
    "    dlt_eps = (avg_d - min_d) * 0.02\n",
    "    min_eps = min_d + dlt_eps / 2\n",
    "    max_eps = min_d + 2 * dlt_eps\n",
    "    \n",
    "    # Find initial nearest neighbor\n",
    "    DK = 1e100\n",
    "    Loc_DK = 1\n",
    "    for i in range(P + 1, M - 1):\n",
    "        d = np.sqrt(np.sum((Y[:, i] - Y[:, 0]) ** 2))\n",
    "        if d < DK and d > min_eps:\n",
    "            DK = d\n",
    "            Loc_DK = i\n",
    "    \n",
    "    # Main loop\n",
    "    sum_lmd = 0\n",
    "    lmd = np.zeros(M - 1)\n",
    "    \n",
    "    for i in range(1, M - 1):\n",
    "        DK1 = np.sqrt(np.sum((Y[:, i] - Y[:, Loc_DK + 1]) ** 2))\n",
    "        old_Loc_DK = Loc_DK\n",
    "        old_DK = DK\n",
    "        \n",
    "        if DK1 != 0 and DK != 0:\n",
    "            sum_lmd += np.log2(DK1 / DK)\n",
    "        \n",
    "        lmd[i - 1] = sum_lmd / i\n",
    "        \n",
    "        # Find new nearest neighbor\n",
    "        point_num = 0\n",
    "        cos_sita = 0\n",
    "        iteration_count = 0\n",
    "        \n",
    "        while point_num == 0:\n",
    "            for j in range(M - 1):\n",
    "                if abs(j - i) <= (P - 1):\n",
    "                    continue\n",
    "                \n",
    "                dnew = np.sqrt(np.sum((Y[:, i] - Y[:, j]) ** 2))\n",
    "                if dnew < min_eps or dnew > max_eps:\n",
    "                    continue\n",
    "                \n",
    "                DOT = np.sum((Y[:, i] - Y[:, j]) * (Y[:, i] - Y[:, old_Loc_DK + 1]))\n",
    "                CTH = DOT / (dnew * DK1) if dnew * DK1 != 0 else 0\n",
    "                CTH = min(CTH, 1.0)\n",
    "                \n",
    "                if math.acos(max(-1, min(1, CTH))) > (np.pi / 4):\n",
    "                    continue\n",
    "                \n",
    "                if CTH > cos_sita:\n",
    "                    cos_sita = CTH\n",
    "                    Loc_DK = j\n",
    "                    DK = dnew\n",
    "                point_num += 1\n",
    "            \n",
    "            if point_num <= min_point:\n",
    "                max_eps += dlt_eps\n",
    "                iteration_count += 1\n",
    "                if iteration_count > MAX_ITERATIONS:\n",
    "                    DK = 1e100\n",
    "                    for ii in range(M - 1):\n",
    "                        if abs(i - ii) <= (P - 1):\n",
    "                            continue\n",
    "                        d = np.sqrt(np.sum((Y[:, i] - Y[:, ii]) ** 2))\n",
    "                        if d < DK and d > min_eps:\n",
    "                            DK = d\n",
    "                            Loc_DK = ii\n",
    "                    break\n",
    "                point_num = 0\n",
    "                cos_sita = 0\n",
    "    \n",
    "    lambda_1 = np.mean(lmd[lmd != 0])\n",
    "    return lambda_1, lmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lyapunov_wolf(series, name, m=1, tau=1, P=5):\n",
    "    \"\"\"Compute and visualize MLE using Wolf algorithm.\"\"\"\n",
    "    lambda_1, lmd = lyapunov_wolf(series, m=m, tau=tau, P=P)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Running MLE estimate\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(lmd, color='#e74c3c', linewidth=1.5)\n",
    "    ax1.axhline(y=lambda_1, color='black', linestyle='--', linewidth=1.5, \n",
    "                label=f'Mean MLE = {lambda_1:.4f}')\n",
    "    ax1.axhline(y=0, color='gray', linestyle='-', linewidth=0.5, alpha=0.5)\n",
    "    ax1.set_xlabel('Iteration', fontsize=11)\n",
    "    ax1.set_ylabel(r'$\\lambda_1$ (bits/iteration)', fontsize=11)\n",
    "    ax1.set_title(f'Wolf Algorithm MLE Estimation: {name}', fontsize=12, fontweight='bold')\n",
    "    ax1.legend(fontsize=10)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, len(lmd))\n",
    "    \n",
    "    # Original series for reference\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(series, color='#3498db', linewidth=1.2)\n",
    "    ax2.set_xlabel('Time', fontsize=11)\n",
    "    ax2.set_ylabel('Value', fontsize=11)\n",
    "    ax2.set_title(f'Original Time Series: {name}', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/lyapunov_wolf_{name.lower().replace(\" \", \"_\")}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Lyapunov Exponent (Wolf): {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"MLE (lambda): {lambda_1:.6f}\")\n",
    "    \n",
    "    return lambda_1\n",
    "\n",
    "# Compute Wolf MLE for both series\n",
    "wolf_online = compute_lyapunov_wolf(df['Online_Shopping'].values, 'Online Shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wolf_mall = compute_lyapunov_wolf(df['Shopping_Mall'].values, 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Entropy\n",
    "\n",
    "Sample Entropy (SampEn) measures the complexity and regularity of a time series. It quantifies the conditional probability that sequences similar for $m$ points remain similar when one additional point is added.\n",
    "\n",
    "**Mathematical Definition:**\n",
    "\n",
    "$$\\text{SampEn}(m, r, N) = -\\ln \\frac{A^m(r)}{B^m(r)}$$\n",
    "\n",
    "where:\n",
    "- $B^m(r)$ = count of template matches of length $m$\n",
    "- $A^m(r)$ = count of template matches of length $m+1$\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower SampEn → Higher regularity/self-similarity → More predictable\n",
    "- Higher SampEn → Higher complexity → Less predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sample_entropy(series, name):\n",
    "    \"\"\"Compute Sample Entropy with visualization.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    sampen = nolds.sampen(series, debug_plot=True, plot_file=ax)\n",
    "    \n",
    "    ax.set_title(f'Sample Entropy Analysis: {name}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/sample_entropy_{name.lower().replace(\" \", \"_\")}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Sample Entropy: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"SampEn: {sampen:.6f}\")\n",
    "    \n",
    "    if sampen < 1.0:\n",
    "        print(f\"\\nInterpretation: LOW entropy indicates HIGH regularity.\")\n",
    "        print(f\"The series exhibits significant self-similarity.\")\n",
    "    else:\n",
    "        print(f\"\\nInterpretation: HIGH entropy indicates LOW regularity.\")\n",
    "        print(f\"The series exhibits complex, less predictable behavior.\")\n",
    "    \n",
    "    return sampen\n",
    "\n",
    "sampen_online = compute_sample_entropy(df['Online_Shopping'].values, 'Online Shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampen_mall = compute_sample_entropy(df['Shopping_Mall'].values, 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hurst Exponent\n",
    "\n",
    "The Hurst exponent measures long-range dependence in time series. It originated from H.E. Hurst's studies of Nile River flooding patterns.\n",
    "\n",
    "**Mathematical Definition (R/S Analysis):**\n",
    "\n",
    "$$\\frac{R(n)}{S(n)} \\propto n^H$$\n",
    "\n",
    "**Interpretation:**\n",
    "- $H = 0.5$: **Random walk** (Brownian motion) - no memory\n",
    "- $0.5 < H < 1$: **Persistent** (trending) - positive autocorrelation\n",
    "- $0 < H < 0.5$: **Anti-persistent** (mean-reverting) - negative autocorrelation\n",
    "\n",
    "**Financial Implications:**\n",
    "- $H > 0.5$: Momentum strategies may be effective\n",
    "- $H < 0.5$: Mean reversion strategies may be effective\n",
    "- $H \\approx 0.5$: Market is efficient (EMH holds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hurst_exponent(series, name):\n",
    "    \"\"\"Compute Hurst exponent using R/S analysis.\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    hurst = nolds.hurst_rs(series, nvals=None, fit='RANSAC', debug_plot=True, plot_file=ax)\n",
    "    \n",
    "    ax.set_title(f'Hurst Exponent (R/S Analysis): {name}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('log(n)', fontsize=11)\n",
    "    ax.set_ylabel('log(R/S)', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/hurst_{name.lower().replace(\" \", \"_\")}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Fractal dimension\n",
    "    fractal_dim = 2 - hurst\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Hurst Exponent: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"H: {hurst:.6f}\")\n",
    "    print(f\"Fractal Dimension (D = 2 - H): {fractal_dim:.6f}\")\n",
    "    \n",
    "    if hurst > 0.5:\n",
    "        print(f\"\\nInterpretation: H > 0.5 indicates PERSISTENT behavior.\")\n",
    "        print(f\"The series exhibits positive long-range dependence (trending).\")\n",
    "        print(f\"Momentum-based strategies may be effective.\")\n",
    "    elif hurst < 0.5:\n",
    "        print(f\"\\nInterpretation: H < 0.5 indicates ANTI-PERSISTENT behavior.\")\n",
    "        print(f\"The series exhibits mean-reverting dynamics.\")\n",
    "        print(f\"Mean reversion strategies may be effective.\")\n",
    "    else:\n",
    "        print(f\"\\nInterpretation: H ≈ 0.5 indicates RANDOM WALK.\")\n",
    "        print(f\"No long-range memory; consistent with EMH.\")\n",
    "    \n",
    "    return hurst, fractal_dim\n",
    "\n",
    "hurst_online, fd_online = compute_hurst_exponent(df['Online_Shopping'].values, 'Online Shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hurst_mall, fd_mall = compute_hurst_exponent(df['Shopping_Mall'].values, 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Spectral Analysis (Fast Fourier Transform)\n",
    "\n",
    "Fourier analysis decomposes a time series into its constituent frequencies, revealing periodic components and dominant cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_analysis(series, name):\n",
    "    \"\"\"Perform FFT-based spectral analysis.\"\"\"\n",
    "    # Compute FFT\n",
    "    n = len(series)\n",
    "    fft_result = np.fft.fft(series)\n",
    "    frequencies = np.fft.fftfreq(n)\n",
    "    \n",
    "    # Get magnitude spectrum (positive frequencies only)\n",
    "    magnitude = np.abs(fft_result)\n",
    "    phase = np.angle(fft_result)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Original signal\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(series, color='#3498db', linewidth=1.2)\n",
    "    ax1.set_xlabel('Time', fontsize=11)\n",
    "    ax1.set_ylabel('Value', fontsize=11)\n",
    "    ax1.set_title(f'Original Time Series: {name}', fontsize=12, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Magnitude spectrum (centered)\n",
    "    ax2 = axes[0, 1]\n",
    "    centered_mag = np.fft.fftshift(magnitude)\n",
    "    centered_freq = np.fft.fftshift(frequencies)\n",
    "    ax2.stem(centered_freq[:n//2], centered_mag[:n//2], linefmt='#e74c3c', markerfmt=' ', basefmt=' ')\n",
    "    ax2.set_xlabel('Frequency', fontsize=11)\n",
    "    ax2.set_ylabel('Magnitude', fontsize=11)\n",
    "    ax2.set_title('Magnitude Spectrum (FFT)', fontsize=12, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Power spectrum\n",
    "    ax3 = axes[1, 0]\n",
    "    power = magnitude ** 2\n",
    "    ax3.semilogy(frequencies[:n//2], power[:n//2], color='#9b59b6', linewidth=1.5)\n",
    "    ax3.set_xlabel('Frequency', fontsize=11)\n",
    "    ax3.set_ylabel('Power (log scale)', fontsize=11)\n",
    "    ax3.set_title('Power Spectrum', fontsize=12, fontweight='bold')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Low-pass filtering reconstruction\n",
    "    ax4 = axes[1, 1]\n",
    "    \n",
    "    for n_components, color, label in [(50, '#e74c3c', '100 components'), \n",
    "                                        (20, '#2ecc71', '40 components'),\n",
    "                                        (10, '#f39c12', '20 components')]:\n",
    "        fft_filtered = fft_result.copy()\n",
    "        fft_filtered[n_components:-n_components] = 0\n",
    "        reconstructed = np.fft.ifft(fft_filtered).real\n",
    "        ax4.plot(reconstructed, color=color, linewidth=1.5, label=label)\n",
    "    \n",
    "    ax4.plot(series, 'k.', markersize=2, alpha=0.5, label='Original')\n",
    "    ax4.set_xlabel('Time', fontsize=11)\n",
    "    ax4.set_ylabel('Value', fontsize=11)\n",
    "    ax4.set_title('Low-Pass Filtered Reconstructions', fontsize=12, fontweight='bold')\n",
    "    ax4.legend(fontsize=9)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'images/spectral_{name.lower().replace(\" \", \"_\")}.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Find dominant frequencies\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Spectral Analysis: {name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Top 5 frequencies by magnitude (excluding DC component)\n",
    "    mag_sorted_idx = np.argsort(magnitude[1:n//2])[::-1] + 1\n",
    "    print(f\"\\nTop 5 dominant frequencies:\")\n",
    "    for i, idx in enumerate(mag_sorted_idx[:5]):\n",
    "        period = 1 / abs(frequencies[idx]) if frequencies[idx] != 0 else np.inf\n",
    "        print(f\"  {i+1}. Frequency: {frequencies[idx]:.4f}, Period: {period:.1f} time units, Magnitude: {magnitude[idx]:.2f}\")\n",
    "\n",
    "spectral_analysis(df['Online_Shopping'].values, 'Online Shopping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_analysis(df['Shopping_Mall'].values, 'Shopping Mall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Phase Space Analysis\n",
    "\n",
    "Phase diagrams visualize the trajectory of a dynamical system in state space. For chaotic systems, these reveal strange attractors—fractal structures toward which the system evolves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYNAMICAL_AVAILABLE:\n",
    "    # Normalize data for phase diagrams\n",
    "    data_mall = pd.DataFrame(df['Shopping_Mall'] / 100)\n",
    "    data_online = pd.DataFrame(df['Online_Shopping'] / 100)\n",
    "    data_combined = pd.concat([data_mall, data_online], axis=1)\n",
    "    data_combined.columns = ['Shopping Mall', 'Online Shopping']\n",
    "    \n",
    "    # 2D Phase diagram (both series)\n",
    "    fig = phase_diagram(data_combined, size=100, \n",
    "                       color=['#3498db', '#2ecc71'], \n",
    "                       ymax=1.005, legend=True)\n",
    "    plt.title('2D Phase Diagram: Shopping Behavior', fontsize=12, fontweight='bold')\n",
    "    plt.savefig('images/phase_diagram_2d.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Phase diagram visualization skipped (pynamical not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PYNAMICAL_AVAILABLE:\n",
    "    # 3D Phase diagram - Shopping Mall\n",
    "    fig = phase_diagram_3d(data_mall, size=50, legend=True)\n",
    "    plt.title('3D Phase Space: Shopping Mall', fontsize=12, fontweight='bold')\n",
    "    plt.savefig('images/phase_diagram_3d_mall.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3D Phase diagram - Online Shopping\n",
    "    fig = phase_diagram_3d(data_online, size=50, legend=True)\n",
    "    plt.title('3D Phase Space: Online Shopping', fontsize=12, fontweight='bold')\n",
    "    plt.savefig('images/phase_diagram_3d_online.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"3D Phase diagram visualization skipped (pynamical not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary of Results\n",
    "\n",
    "Let's compile all chaos metrics into a comprehensive summary table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = {\n",
    "    'Metric': [\n",
    "        'MLE (Rosenstein)',\n",
    "        'MLE (Wolf)',\n",
    "        'Lyapunov Time',\n",
    "        'Hurst Exponent',\n",
    "        'Fractal Dimension',\n",
    "        'Sample Entropy',\n",
    "        'BDS Statistic',\n",
    "        'BDS p-value',\n",
    "        'ADF Statistic',\n",
    "        'ADF p-value'\n",
    "    ],\n",
    "    'Online Shopping': [\n",
    "        f\"{mle_online:.6f}\",\n",
    "        f\"{wolf_online:.6f}\",\n",
    "        f\"{1/mle_online:.2f}\" if mle_online > 0 else \"N/A\",\n",
    "        f\"{hurst_online:.4f}\",\n",
    "        f\"{fd_online:.4f}\",\n",
    "        f\"{sampen_online:.4f}\",\n",
    "        f\"{bds_online[0]:.4f}\",\n",
    "        f\"{bds_online[1]:.2e}\",\n",
    "        f\"{adf_online[0]:.4f}\",\n",
    "        f\"{adf_online[1]:.4f}\"\n",
    "    ],\n",
    "    'Shopping Mall': [\n",
    "        f\"{mle_mall:.6f}\",\n",
    "        f\"{wolf_mall:.6f}\",\n",
    "        f\"{1/mle_mall:.2f}\" if mle_mall > 0 else \"N/A\",\n",
    "        f\"{hurst_mall:.4f}\",\n",
    "        f\"{fd_mall:.4f}\",\n",
    "        f\"{sampen_mall:.4f}\",\n",
    "        f\"{bds_mall[0]:.4f}\",\n",
    "        f\"{bds_mall[1]:.2e}\",\n",
    "        f\"{adf_mall[0]:.4f}\",\n",
    "        f\"{adf_mall[1]:.4f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df.set_index('Metric', inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CHAOS ANALYSIS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Interpretation and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on our chaos theory analysis of the Google Trends shopping behavior data:\n",
    "\n",
    "1. **Nonlinearity (BDS Test)**:\n",
    "   - Both series strongly reject the null hypothesis of i.i.d., confirming significant nonlinear structure.\n",
    "   - This suggests the time series contain deterministic patterns beyond linear autocorrelation.\n",
    "\n",
    "2. **Stationarity (ADF Test)**:\n",
    "   - Both series are non-stationary (have unit roots), indicating persistent trends.\n",
    "   - This is consistent with the visible upward trend in online shopping and seasonal patterns.\n",
    "\n",
    "3. **Chaos (Lyapunov Exponents)**:\n",
    "   - Positive MLE values indicate sensitivity to initial conditions.\n",
    "   - The Lyapunov time suggests limited predictability horizons.\n",
    "\n",
    "4. **Long-Range Dependence (Hurst Exponent)**:\n",
    "   - $H > 0.5$ for both series indicates persistent (trending) behavior.\n",
    "   - Past trends tend to continue, suggesting momentum effects.\n",
    "\n",
    "5. **Complexity (Sample Entropy)**:\n",
    "   - Moderate entropy values indicate neither perfectly regular nor completely random behavior.\n",
    "   - The series exhibit complex but structured dynamics.\n",
    "\n",
    "### Implications for Financial Analysis\n",
    "\n",
    "While this dataset represents consumer behavior rather than financial returns, similar chaos analysis methods can be applied to financial time series:\n",
    "\n",
    "- **Positive Lyapunov exponents** in financial data suggest inherent limits to predictability\n",
    "- **Hurst exponents** can inform trading strategy selection (momentum vs. mean reversion)\n",
    "- **BDS tests** on model residuals can detect remaining nonlinear structure\n",
    "- **Regime changes** (like COVID-19 impact) demonstrate sensitive dependence in real-world systems\n",
    "\n",
    "### Caveats\n",
    "\n",
    "- Results depend on embedding parameters (dimension, delay)\n",
    "- Sample sizes may be insufficient for precise estimation\n",
    "- Nonstationarity complicates interpretation of chaos metrics\n",
    "- Noise can bias Lyapunov exponent estimates upward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Rosenstein, M. T., Collins, J. J., & De Luca, C. J. (1993). A practical method for calculating largest Lyapunov exponents from small data sets. *Physica D*, 65(1-2), 117-134.\n",
    "\n",
    "2. Wolf, A., Swift, J. B., Swinney, H. L., & Vastano, J. A. (1985). Determining Lyapunov exponents from a time series. *Physica D*, 16(3), 285-317.\n",
    "\n",
    "3. Hurst, H. E. (1951). Long-term storage capacity of reservoirs. *Transactions of the American Society of Civil Engineers*, 116(1), 770-799.\n",
    "\n",
    "4. Richman, J. S., & Moorman, J. R. (2000). Physiological time-series analysis using approximate entropy and sample entropy. *American Journal of Physiology*, 278(6), H2039-H2049.\n",
    "\n",
    "5. Brock, W. A., Scheinkman, J. A., Dechert, W. D., & LeBaron, B. (1996). A test for independence based on the correlation dimension. *Econometric Reviews*, 15(3), 197-235.\n",
    "\n",
    "---\n",
    "\n",
    "*Notebook created with [nolds](https://github.com/CSchoel/nolds) and [pynamical](https://github.com/gboeing/pynamical)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
